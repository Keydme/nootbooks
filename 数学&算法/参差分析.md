## 熵权法
### 简单原理
基于一组数据，方差越小，说明该数据的相似度越大，我们可以判断其优劣的贡献度越小。

极端的来看，某项指标，每个对象的得分相同，数据的相似度达到最大，则判断哪个对象较好是没有必要的，即说明这个指标的权重为0.
熵是对不确定性的一种度量，从某种角度来看：

不确定性越大，熵就越大，包含的信息量越大，对应的权重越高；

不确定性越小，熵就越小，包含的信息量越小，对应的权重越低。
### 指标的正向化（化成共性指标，即越大越好，以便后续的统一操作）
亦可不做相关处理直接归一化
|指标名称|指标特点|例子|
|--|--|--|
|极大型指标|越大越好|成绩、利润、GDP增速|
|极小型指标|越小越好|花费、污染程度、失业率|
|中间型指标|越接近某个值越好|水质量评估时的PH值|
|区间型指标|落在某个区间最好|体温、水中某物质含量|
将所有的指标转化为极大型指标，称为指标的正向化

- 极小->极大:
$$x_i=max\{x_1,x_2,\dots,x_i\}-x_i$$
若全为正可取倒数
$$x_i=1/x_i$$
- 中间->极大：
	先转为极小再极大
$$M=max\{|x_i-x_{best}|\}$$
$$x_{new}=1-\frac{|x_i-x_{best}|}{M}$$
- 区间型：
	等同极大
### 标准化（归一化）处理（平衡因为指标之间的差异或是量纲带来的误差）
于各项指标的计量单位并不统一，因此在用它们计算综合指标前，先要进行标准化处理，即把指标的绝对值转化为相对值，从而解决各项不同质指标值的同质化问题。
另外，正向指标和负向指标数值代表的含义不同（正向指标数值越高越好，负向指标数值越低越好），因此，对于正向负向指标需要采用不同的算法进行数据标准化处理：
##### 极差变换法
#极差变换法
正向指标:
$$x_{ij} = \frac{x_{ij}-min\{x_{1j},\dots,x_{ij}\}}{max\{x_{1j},\dots,x_{ij}\}-min\{x_{1j},\dots,x_{ij}\}}$$
负向指标:
$$x_{ij} = \frac{max\{x_{1j},\dots,x_{ij}\}-x_{ij}}{max\{x_{1j},\dots,x_{ij}\}-min\{x_{1j},\dots,x_{ij}\}}$$
```python
def normalize(matr_df,flag=1):
    arr = []
    for col in matr_df.columns:
        r = max(matr_df[col]),min(matr_df[col])
        s = flag*(matr_df[col]-r[flag])/(r[0]-r[1])
        arr.append(s.values)
    res=np.stack(arr).T
    res=pd.DataFrame(res,columns=matr_df.columns,index=matr_df.index)
    return res
```
##### Z-Score法
#Z-Score法
Z-Score标准化并不会将数据放缩在0-1之间，而是均匀地分布在0的两侧。类似这种数据也被称为Zero-Centered Data，在深度学习领域有重要应用。当然我们可以将上述过程封装为一个函数：
同样是逐列进行操作，每一条数据都减去当前列的均值再除以当前列的标准差。很明显，通过这种方法处理之后的数据是典型的Zero-Centered Data，
并且如果原数据服从正态分布，通过Z-Score处理之后将服从标准正态分布。
$$X_{normalization} = \frac{x-\mu}{\sigma}$$
其中$\mu$为均值，$\sigma$为标准差

### 每个元素所占比重（比重之和为1，即对标准化后的矩阵归一化）
概率矩阵p：
$$p_{ij}=\frac{z_{ij}}{\sum^{n}_{i=1}z_{ij}}$$
### 计算信息熵e（不确定度）
$$e_j=-k\sum^{n}_{i=1}p_{ij}ln(p_{ij}),(j = 1,2,3,\dots,m)$$
其中
$$k = \frac{1}{lnn}>0,e_j>0$$
### 计算权重
$$d_j=1-e_j$$
$$w_j = \frac{d_j}{\sum^m_{j=1}d_j},(j=1,2,3,\dots,m)$$
```python
def EWM(data):
    k = 1/np.log(len(data))
    p = data.apply(lambda x:x/x.sum())
    logp = np.log(p.values)
    logp[np.where(np.isinf(logp))] = 0
    logp = pd.DataFrame(logp,columns=p.columns,index=p.index)
    e =(logp*p).apply(lambda x:-k*sum(x))
    # e = pd.DataFrame(e,index=p.index)
    d = 1-e
    w = d/d.sum()
    return w
```
### 综合得分
权重\*得分
$$score_i = \sum^m_{j=1}w_jz_{ij},(i = 1,2,\dots,n)
```python

```